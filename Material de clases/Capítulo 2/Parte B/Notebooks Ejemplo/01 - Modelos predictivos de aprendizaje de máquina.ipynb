{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<font size='5' face='Georgia, Arial'>IIC2115 - Programación como herramienta para la ingeniería</font><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos predictivos basados en aprendizaje de máquina\n",
    "\n",
    "Uno de los objetivos principales del análisis de datos es la construcción de modelos predictivos basados en datos, que permitan inferir el comportamiento de un fenómeno en el futuro. En esta parte del capítulo nos centraremos en técnicas de aprendizaje de máquina para construir estos modelos predictivos basados en datos. Para esto, utilizaremos la librería `scikit-learn`, que la más utilizada para este tipo de análisis.\n",
    "\n",
    "Antes de describir los modelos y el uso de la librería, presentaremos una breve introducción a los conceptos teóricos esenciales del aprendizaje de máquina. En particular, describiremos los distintos tipos de aprendizaje que existen, el mecanismo de entrenamiento de un modelo y como se evalúa su comportamiento a través conjuntos de validación y métricas de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es el aprendizaje de máquina?\n",
    "\n",
    "El aprendizaje de máquina, o _Machine Learning_ (ML) en inglés, es una disciplina que combina conocimientos y técnicas de ciencia de la computación, cálculo y estadística, donde se busca construir modelos de análisis y procesamiento de datos que permitan resolver problemas específicos. Estos modelos cumplen en general las siguientes características:\n",
    "* Modelos (principalmente) predictivos basados en datos, enfocados en problemas específicos.\n",
    "* Mejoran con la experiencia/datos (generalmente, mientras más, mejor).\n",
    "* Buscan aprendizaje más que modelamiento de datos.\n",
    "\n",
    "Estas características, unidas a su buen rendimiento en problemas reales y a la gran cantidad de librerías relacionadas disponibles, hacen que las técnicas de _machine learning_ sean ampliamente usadas para analizar datos. Algunos ejemplos de uso del aprendizaje de máquina son los detectores automáticos de fraude bancario, los filtros anti-spam, los sistemas recomendadores y los sistemas de traducción entre idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de aprendizaje de máquina\n",
    "\n",
    "Si bien es posible definir más tipos, en general las técnicas y modelos de _machine learning_ pueden corresponder a tres tipos distintos de aprendizaje: supervisado, no supervisado y reforzado: \n",
    "\n",
    "<img src=\"figs/ml_categories.png\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "El aprendizaje supervisado es el más común e incluye problemas como regresión y clasificación. En este esquema, el objetivo es predecir para cada dato ya sea un valor numérico (regresión) o una etiqueta (clasificación). Para lograr esto, se cuenta con un conjunto de datos donde estos valores son conocidos, a partir de los cuales estima un modelo predictivo que idealmente sea capaz de generalizar a datos no vistos anteriormente. Por ejemplo, si los datos consisten en información de las condiciones meteorológicas, incluída la temperatura, para todos los días del año 2020, podríamos construir un modelo de aprendizaje supervisado que, dadas las condiciones meteorológicas de cualquier día del año 2021, prediga la temperatura para el día siguiente.\n",
    "\n",
    "El aprendizaje no supervisado se centra en el descrubrimiento de estructuras o relaciones en los datos, sin contar con etiquetas o valores a predecir, como en el aprendizaje supervisado. Ejemplos de tareas de aprendizaje no supervisado son el _clustering_ y la reducción de dimensionalidad.\n",
    "\n",
    "Finalmente, el aprendizaje reforzado considera la existencia de un *agente* que debe interactuar con un *entorno*, con el fin de cumplir una tarea. A diferencia del aprendizaje supervisado, no existen etiquetas que indiquen lo que se debe hacer, sino que el agente recibe una recompensa (relacionada con la tarea) al interactuar con el ambiente, y así este buscará maximizar esta recompensa en el tiempo. Ejemplos de dominios donde se aplica el aprendizaje reforzado son el control de personajes en juegos o los procesos de decisión con incerteza.\n",
    "\n",
    "En este curso nos centraremos en los modelos y técnicas de aprendizaje supervisado. Si está interesado en aprender más sobre estos y otros tópicos relacionados, se recomienda tomar en el futuro cursos como Inteligencia Artificial (IIC2613) o Sistemas Urbanos Inteligentes (ICT3115)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos\n",
    "\n",
    "Como norma general, los modelo de _machine learning_ trabajan sobre datos multidimensionales, es decir, cada dato es considerado como un vector multidimensional, donde cada dimensión corresponde a un atributo o variable. Por ejemplo, si tenemos un `DataFrame`, podemos considerar que cada fila es un vector, donde cada columna representa una de las dimensiones del dato. Si bien este es un caso común, los modelos de ML pueden trabajar con otro tipo de datos multidimensionales, como imágenes o grafos. Para referirse al espacio vectorial donde viven los datos, generalmente se utiliza el término espacio de característica, o _feature space_.\n",
    "\n",
    "La estimación de los parámetros de un modelo se realiza a través de un proceso llamado **entrenamiento**. En él, los datos son divididos en dos conjuntos disjuntos: conjuto (o set) de entrenamiento, y conjunto (o set) de prueba (o test). Los detalles de cómo exactamente se entrena cada modelo exceden al alcance del curso, pero todos siguen aproximadamente el siguiente proceso. Dado que el set de entrenamiento en un conjunto acotado de datos, se asumen inicialmente que este es suficientemente representativo de la distribución real de los datos. Dado esto, se itera sobre este conjunto de manera que las predicciones hechas por el modelo sean progresivamente mejores, es decir, más cercanas a las etiquetas o valores a predecir. Para muchos modelos, este proceso de mejora iterativa es planteado y resuelto como un problema de optimización, donde se debe minimizar una función de pérdida o costo, que mide la discrepancia entre las predicciones y los valores originales a predecir. \n",
    "\n",
    "A diferencia de otros modelos estadísticos, los modelos de ML consideran la evaluación de su rendimiento a través del cálculo de una métrica de error (o éxito) en el conjunto de test. Dado que los ejemplos en el set de test no fueron utilizados en el entrenamiento, un alto rendimiento del modelo en él asegura que este es capaz de generalizar a casos no vistos previamente, lo que da garantía de su robustez.\n",
    "\n",
    "Otro de los motivos por el cual se utiliza un set de datos independiente para mejor el rendimiento, es debido a un fenómeno conocido como **sobreentrenamiento** u _overfitting_, en donde el modelo memoriza los ejemplos que procesó en el set de entrenamiento, lo que redunda en un rendimiento casi perfecto en este conjunto de datos. Sin embargo, al evaluar el rendimiento en el set de prueba, este suele ser notoriamente menor que el obtenido en el set de entrenamiento. Esta situación se genera la mayoría de las veces por: i) realizar una cantidad excesiva de iteraciones de entrenamiento y/o ii) por tener un modelo de mayor complejidad a la necesaria para resolver el problema (sobreparametrizado). Para combatir esta situación, se utiliza generalmente un tercer conjunto de datos conocido como set de validación. Este set es un subconjunto del set de entrenamiento y se utiliza para evaluar (validar) constantemente el rendimiento del modelo mientras se entrena, con la particularidad de que sus datos no son utilizados en el entrenamiento. De esta forma, el rendimiento en el set de validación permite estimar como cambia el rendimiento en el set de test a medida que se realiza el entrenamiento, lo que da la posibilidad de detener el proceso cuando se vea una detención en la mejora del rendimiento. Si bien este esquema es muy utilizado por su simpleza, existen otros mecanismos más robustos como _validación cruzada_ o _leave-one-out validation_. La siguiente figura muestra graficamente la divisiones realizadas en un set de datos para generar los conjuntos de entrenamiento, validación y test:\n",
    "\n",
    "<img src=\"figs/datasets.png\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "Finalmente, tal como existe sobreentrenamiento, es posible tener problemas de subentrenamiento o _underfitting_, donde la capacidad del modelo no es suficiente para capturar el fenómeno a predecir. Un caso típico de _underfitting_ y _overfitting_ se puede apreciar en la siguiente figura, donde se muestra el resultado de estimar una función cuadrática donde los datos tiene ruido:\n",
    "\n",
    "Underfitting | Good fit | Overfitting\n",
    ":-: | :-: | :-:\n",
    "<img src=\"figs/underfitting.png\" width=\"300px\" align=\"center\"/> | <img src=\"figs/good_fit.png\" width=\"300px\" align=\"center\"/> | <img src=\"figs/overfitting.png\" width=\"270px\" align=\"center\"/>\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de aprendizaje supervisado\n",
    "\n",
    "Como se mencionó anteriormente, existen múltiples modelos de aprendizaje supervisado, cada uno con sus características propias, ventajas y desventajas que los hacen más o menos adecuados a los disintos tipos de problemas de datos que encontremos. A continuación se describirán las características de algunos de ellos.\n",
    "\n",
    "### K-vecinos cercanos (KNN)\n",
    "Este algoritmo consiste en realizar predicciones basadas unicamente en los K elementos más cercanos al dato analizado, en el espacio de características. Si la tarea es una clasificación, el resultado se entrega a través de una votación de mayoría (las clase más repetida entre los K vecinos gana) o un promedio si la tarea es una regresion (promedio de los K vecinos). Al solo basarse en el cálculo de distancias, este modelo no necesita entrenamiento, lo que lo hace muy simple y rápido de utilizar. Sin embargo, su rendimiento en cuanto a tiempo de ejecución sufre mucho cuando la cantidad de ejemplos en el set de entrenamiento es alta (piense en la complejidad computación de calcular la distancia euclidiana entre el punto analizado y todos los ejemplos de entrenamiento y luego ordenarlas). Otra desventaja es la sensibilidad al hiperparámetro K, que altera fuertemente el rendimiento.\n",
    "\n",
    "<img src=\"figs/knn.png\" width=\"300px\" align=\"center\"/>\n",
    "\n",
    "### Regresión lineal, logística y polinomial\n",
    "Estas técnicas de aprendizaje estadístico permiten estimar funciones (reg. lineal) o clasificar (reg. logística) en base a una combinación lineal de las características de los datos. Además, si se expanden las características utilizando una transformación polinomial de estas, es posible hacer estimaciones de funciones y clasificación no lineal. Estas técnicas son ampliamente utilizadas debido a su rapidez, sencillez e interpretabilidad, pero tienen problemas para obtener buenos rendimientos en problemas complejos.\n",
    "\n",
    "Regresión lineal | Regresión logística | Regresión polinomial\n",
    ":-: | :-: | :-:\n",
    "<img src=\"figs/linear_regression.png\" width=\"300px\" align=\"center\"/> | <img src=\"figs/logistic_regression.png\" width=\"290px\" align=\"center\"/> | <img src=\"figs/polynomial_regression.png\" width=\"250px\" align=\"center\"/>\n",
    "\n",
    " \n",
    "\n",
    "### Support Vector Machine\n",
    "Un Support Vector Machines (SVM) permite hacer clasificación binaria o multiclase, utilizando el concepto de máximo margen. En él, se busca el clasificador lineal (hiperplano) que maximice la distancia entre las clases. Esto permite obtener un poder de generalización más alta que otros métodos como la regresión lineal. Además del rendimiento, los SVM son muy rápidos de entrenar, lo que los hace ser candidatos perfectos cuando se tiene gran volumen de datos.\n",
    "\n",
    "<img src=\"figs/svm.png\" width=\"250px\" align=\"center\"/>\n",
    "\n",
    "### Árboles de decisión y regresión\n",
    "Los árboles de decisión son técnicas simples de clasificación o regresión, que funcionan con cualquier tipo de atributo (numéricos, categóricos, binarios, etc). Su funcionamiento se basa en la construcción  de una estructura de árbol, en base a tests/umbrales calculados para cada característica analizada. Esta árbol es construido hasta llegar a los nodos hoja, donde se entrega la predicción o clasificación. Si bien sufren serios problemas de _overfitting_, su alto nivel de explicabilidad los hace siempre buenos candidatos, sobre todo si se tiene gran cantidad de datos.\n",
    "\n",
    "<img src=\"figs/decision_tree.png\" width=\"300px\" align=\"center\"/>\n",
    "\n",
    "### Ensambles de árboles\n",
    "Lás técnicas de ensamble se basan en la combinación de múltiples árboles para generar una predicción más robusta y menos sesgada. Si bien existen múltiples existen diversas maneras de combinar estas predicciones, las estrategias paralela aleatoria (_Random Forest_) y secuencial (_Gradient Boosting_) son las que mejores resultados obtienen en la práctica. Estas técnicas son en la actualidad de las más competiticas en cuanto a rendimiento al utilizar datos tabulados.\n",
    "\n",
    "<img src=\"figs/ensemble.png\" width=\"330px\" align=\"center\"/>\n",
    "\n",
    "### Redes Neuronales\n",
    "Las redes neuronales son modelos altamente generales y complejos para estimar funciones de cualquier tipo (de hecho, es posible demostrar que pueden aprender cualquier función). Su principal característica es que procesan los datos a través de varias capas, que combinan operaciones lineales y no lineales. Si se tienen muchos datos, estos modelos son por lejos los que mejor funcionan, en particular para datos no estructurados (imágenes, texto, audio, video, etc.)\n",
    "\n",
    "<img src=\"figs/nn.png\" width=\"300px\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de aprendizaje de máquina en Python\n",
    "\n",
    "Como se mencionó anteriorment, utilizaremos la librería `scikit-learn` para aplicar técnicas de ML al procesamiento de datos. La popularidad de esta librería radica principalmente en el uso de una interfaz limpia, uniforme y simple, que facilita la exploración y permite la integración con otras librerías, como Pandas. En otras palabras, todos los modelos y técnicas disponibles funcionan _out-of-the-box_ y utilizando una interfaz de ejecución y configuración casi idéntica para todos. Esto permite además obviar parcialmente los detalles teóricos de los modelos y concentrarse en la exploración y construcción de soluciones.\n",
    "\n",
    "Para utilizar `scikit-learn` es necesario inicialmente instalar la librería con pip. Es muy probable que ya esté instalada, sobre todo si se utiliza Colab, pero en caso que no lo esté, el código en la siguiente celda se encarga de hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esquema de representación de datos\n",
    "El esquema de representación de datos de `scikit-learn` es sumamente directo y sencillo. Salvo algunos casos particulares, todos los modelos recibirán como entrada para entrenamiento y evaluación, una matriz de features y un vector objetivo. La matriz de features, o **X**, tiene dimensiones `n_ejemplos X n_features` y puede ser representada por un `DataFrame` (o un `array` de `Numpy`). El vector objetivo, o **y**, contiene los valores a predecir para cada ejemplo (sean estos números o categorías), tiene dimensiones de `n_ejemplos X 1` y puede ser representado por una `Series` (o un `array` de `Numpy`).\n",
    "\n",
    "Dependiendo del caso, la matriz *X* podrá representará el set de entrenamiento, de validación o de test, pero siempre tendrá el mismo formato.\n",
    "\n",
    "<img src=\"figs/sklearn_data_model.png\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "El siguiente ejemplo muestra como cargar un conjunto de datos y luego crear sets de datos independientes para entrenamiento, test y validación, utilizando las funcionalidades de `scikit-learn`. En particular, utilizaremos el set de datos [`Iris`](https://archive.ics.uci.edu/ml/datasets/iris). Este conjunto de datos es ideal para practicar conceptos de ML, debido a su tamaño y simplicidad. El set `Iris` contiene 150 ejemplos pertenecientes a 3 clases de plantas de la familia Iris, donde cada ejemplo está descrito por 4 características. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "iris_dataset = pd.read_csv('iris.csv')\n",
    "iris_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de poder construir los modelos predictivos, es necesario crear la matriz **X** y el vector **y** para cada caso. Además de esto, `scikit-learn` requiere que todas las _features_ sean numéricas, por lo que de deben transformar todas las variables categóricas en el set de datos (en este caso solo `species`) a numéricas, codificando las categorías como números naturales. Esto se puede hacer usando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_vars = ['species']\n",
    "label_encoder = LabelEncoder()\n",
    "for i in cat_vars:\n",
    "    iris_dataset[i] = label_encoder.fit_transform(iris_dataset[i])\n",
    "iris_dataset.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante que el paso anterior se realice siempre antes de la creación de los sets de entrenamiento, validación y test. De esta manera, se puede asegurar la consistencia en la codificación de las variables categóricas para todos los conjuntos (si no, podría ser que la especie _setosa_ en uno de los conjuntos sea codificada como un 1 y en otro como un 0).\n",
    "\n",
    "A continuación, se utiliza la función `train_test_split` de `scikit-learn` para crear los conjuntos de datos respectivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, test_set = train_test_split(iris_dataset.copy(), test_size = 0.3)\n",
    "training_set, validation_set = train_test_split(training_set.copy(), test_size = 0.1)\n",
    "\n",
    "print(f'Tamaño set entrenamiento: {len(training_set)}')\n",
    "print(f'Tamaño set validación: {len(validation_set)}')\n",
    "print(f'Tamaño set test: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible, y muchas veces recomendable, realizar una etapa de preprocesamiento de los datos antes de entregarlos a los modelos predictivos. Por ejemplo, es común normalizar cada _feature_ con el fin de que todas se encuentren en la misma escala. Sin embargo, este proceso requiere bastante cuidado ya que es muy fácil caer en el problema de _data leakage_, que consiste en incluir de manera indirecta en el set de entrenamiento información que pertenece al set de test. Un ejemplo típico de esto es usar un proceso de estandarización para normalizar las _features_, calculando la media y varianza en base al conjunto completo de los datos (en este caso `iris_dataset`). El procedimiento correcto es calcular estos coeficientes en el set de entrenamiento (en este caso `training_set`) y luego usarlos para estandarizar los datos en el resto de los conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features = ['sepal_length','sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "training_set[features] = scaler.fit_transform(training_set[features])\n",
    "validation_set[features] = scaler.transform(validation_set[features])\n",
    "test_set[features] = scaler.transform(test_set[features])\n",
    "\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de modelos predictivos\n",
    "\n",
    "Con los datos ya preparado, podemos empezar el proceso de entrenamiento de los modelos. Para facilitar esto, definiremos una función que realice entrenamiento y evaluación de rendimiento en un set separado, para cualqueir modelo que se le entregue como parámetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def training_and_eval(model, training, eval, features, target):\n",
    "    model.fit(training[features], training[target])\n",
    "    predictions = model.predict(eval[features])\n",
    "    accuracy = metrics.accuracy_score(predictions, eval[target])\n",
    "    print(f\"Accuracy: {accuracy: .2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica de rendimiento utilizada en este caso es _accuracy_, que corresponde a la razón entre aciertos y la cantidad de ejemplos procesados. Cuando las clases no están balanceadas, se recomienda utilizar la métrica _balanced accuracy_. Queda como ejercicio propuesto realizar este ejercicio para una tarea de regresión.\n",
    "\n",
    "Teniendo todo esto, podemos comenzar a probar distintos modelos. Para este ejemplo evaluaremos 3: árbol de decisión, SVM y KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "target = 'species'\n",
    "model = DecisionTreeClassifier()\n",
    "training_and_eval(model, training_set, test_set, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "training_and_eval(model, training_set, test_set, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "training_and_eval(model, training_set, test_set, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el set de datos es pequeño y simple, el rendimiento de todos los modelos es excelente. En vista de eso, queda como ejercicio propuesto explorar más sets de datos (hay muchos en `scikit-learn`) y evaluar su rendimiento considerando el uso de un set de validación para controlar el _overfitting_. Para esto último, explore los distintos hiperparámetros que tiene cada modelo, con el fin de considerar como afectan estos a la complejidad de los modelos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iic2115')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c351642c1ff32041210c09392ed7c7736f797ffa83d23e3a89bc83a72472dbd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
